{
  "id": "ml-80",
  "title": "Classification Metrics Deep Dive",
  "category": "Classification Algorithms",
  "number": 80,
  "duration": "15 min read",
  "content": "## Classification Metrics Deep Dive\n\nAccuracy is not always the best metric, especially with imbalanced datasets. We need more robust metrics.\n\n### Confusion Matrix\n\nA table showing True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN).\n\n### Key Metrics\n\n1.  **Precision**: $TP / (TP + FP)$. Of all predicted positives, how many are actually positive? Important when False Positives are costly (e.g., email spam).\n2.  **Recall (Sensitivity)**: $TP / (TP + FN)$. Of all actual positives, how many did we catch? Important when False Negatives are costly (e.g., cancer detection).\n3.  **F1-Score**: Harmonic mean of Precision and Recall. $2 * (Precision * Recall) / (Precision + Recall)$.\n4.  **ROC Curve**: Plots True Positive Rate (Recall) vs False Positive Rate at various thresholds.\n5.  **AUC (Area Under Curve)**: Measures the entire 2D area underneath the ROC curve. 1.0 is perfect, 0.5 is random guessing.\n\n### Classification Report\n\n```python\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n```\n",
  "code": "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\n# Create imbalanced dataset\nX, y = make_classification(n_samples=1000, n_classes=2, weights=[0.9, 0.1], random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\n# TODO: Print Confusion Matrix\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n\n# TODO: Print Classification Report\nprint(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n\n# TODO: Calculate ROC AUC Score (requires probabilities)\ny_prob = model.predict_proba(X_test)[:, 1]\nprint(f\"ROC AUC Score: {roc_auc_score(y_test, y_prob):.4f}\")"
}