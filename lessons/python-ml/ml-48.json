{
  "id": "ml-48",
  "title": "Bias-Variance Tradeoff",
  "category": "ML Fundamentals",
  "number": 48,
  "duration": "12 min read",
  "content": "## The Fundamental Tension in ML\n\nThe Bias-Variance Tradeoff describes the balance between two sources of error that prevent supervised learning algorithms from generalizing beyond their training set.\n\n### Bias (Underfitting)\n\nError due to erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting).\n- **High Bias**: Model is too simple.\n\n### Variance (Overfitting)\n\nError due to too much sensitivity to small fluctuations in the training set. High variance can cause an algorithm to model the random noise in the training data rather than the intended outputs (overfitting).\n- **High Variance**: Model is too complex.\n\n### The Tradeoff\n\n- **Low Complexity**: High Bias, Low Variance.\n- **High Complexity**: Low Bias, High Variance.\n\nThe goal is to find the sweet spot where the total error (Bias + Variance + Irreducible Error) is minimized.\n",
  "code": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\n\n# Generate non-linear data\nnp.random.seed(0)\nX = np.sort(np.random.rand(20))\ny = np.cos(1.5 * np.pi * X) + np.random.randn(20) * 0.1\n\n# Helper to fit and plot\ndef fit_plot(degree, ax):\n    model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n    model.fit(X[:, np.newaxis], y)\n    \n    X_test = np.linspace(0, 1, 100)\n    y_pred = model.predict(X_test[:, np.newaxis])\n    \n    ax.scatter(X, y, color='black', s=20, label='Data')\n    ax.plot(X_test, y_pred, label=f'Degree {degree}')\n    ax.set_title(f'Degree {degree}')\n    ax.legend()\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\nfit_plot(1, axes[0])   # High Bias (Underfitting)\nfit_plot(4, axes[1])   # Balanced\nfit_plot(15, axes[2])  # High Variance (Overfitting)\nplt.show()"
}
