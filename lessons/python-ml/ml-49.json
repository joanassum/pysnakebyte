{
  "id": "ml-49",
  "title": "Overfitting and Underfitting",
  "category": "ML Fundamentals",
  "number": 49,
  "duration": "10 min read",
  "content": "## Diagnosing Model Performance\n\nUnderstanding whether your model is overfitting or underfitting is critical for improving it.\n\n### Underfitting\n\nOccurs when the model is not complex enough to capture the underlying structure of the data.\n- **Symptoms**: Poor performance on both training and test sets.\n- **Solutions**: Increase model complexity, add more features, remove regularization.\n\n### Overfitting\n\nOccurs when the model learns the detail and noise in the training data to the extent that it negatively impacts the performance of the model on new data.\n- **Symptoms**: Excellent performance on training set, poor performance on test set.\n- **Solutions**: Get more data, simplify the model, use regularization (L1/L2), early stopping, cross-validation.\n\n### Learning Curves\n\nPlotting training and validation scores against the number of training examples can help diagnose these issues.\n",
  "code": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.svm import SVC\nfrom sklearn.datasets import load_digits\n\ndigits = load_digits()\nX, y = digits.data, digits.target\n\n# Create a model\nmodel = SVC(gamma=0.001)\n\n# Calculate learning curve\ntrain_sizes, train_scores, test_scores = learning_curve(\n    model, X, y, cv=5, n_jobs=-1,\n    train_sizes=np.linspace(0.1, 1.0, 5)\n)\n\ntrain_scores_mean = np.mean(train_scores, axis=1)\ntest_scores_mean = np.mean(test_scores, axis=1)\n\n# Plot\nplt.figure(figsize=(8, 5))\nplt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\nplt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\nplt.title(\"Learning Curve\")\nplt.xlabel(\"Training examples\")\nplt.ylabel(\"Score\")\nplt.legend(loc=\"best\")\nplt.grid()\nplt.show()"
}
