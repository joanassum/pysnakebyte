{
  "id": "ml-83",
  "title": "Hierarchical Clustering",
  "category": "Unsupervised Learning",
  "number": 83,
  "duration": "15 min read",
  "content": "## Hierarchical Clustering\n\nHierarchical clustering builds a hierarchy of clusters. It doesn't require pre-specifying the number of clusters.\n\n### Types\n\n1.  **Agglomerative (Bottom-Up)**: Start with each point as its own cluster. Merge the two closest clusters. Repeat until only one cluster remains.\n2.  **Divisive (Top-Down)**: Start with all points in one cluster. Split recursively.\n\n### Dendrogram\n\nA tree-like diagram that records the sequences of merges or splits. The vertical axis represents the distance between clusters. By cutting the dendrogram at a certain height, we can determine the number of clusters.\n\n### Linkage Methods\n\nHow to measure distance between clusters?\n*   **Ward**: Minimizes the variance of the clusters being merged.\n*   **Single**: Distance between the closest points of two clusters.\n*   **Complete**: Distance between the farthest points of two clusters.\n*   **Average**: Average distance between all points.\n\n### Implementation\n\n```python\nfrom sklearn.cluster import AgglomerativeClustering\nimport scipy.cluster.hierarchy as sch\n\n# Create Dendrogram\ndendrogram = sch.dendrogram(sch.linkage(X, method='ward'))\n\n# Fit model\nhc = AgglomerativeClustering(n_clusters=5, affinity='euclidean', linkage='ward')\ny_hc = hc.fit_predict(X)\n```\n",
  "code": "import scipy.cluster.hierarchy as sch\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.datasets import make_blobs\nimport matplotlib.pyplot as plt\n\nX, y = make_blobs(n_samples=50, centers=3, random_state=42)\n\n# TODO: Plot the dendrogram\n# plt.figure(figsize=(10, 7))\n# plt.title(\"Dendrogram\")\n# dendrogram = sch.dendrogram(sch.linkage(X, method='ward'))\n# plt.show()\n\n# TODO: Initialize AgglomerativeClustering with 3 clusters\nhc = \n\n# TODO: Fit and predict\n# y_hc = \n\n# print(y_hc)"
}