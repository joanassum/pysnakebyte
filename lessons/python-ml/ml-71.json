{
  "id": "ml-71",
  "title": "Logistic Regression",
  "category": "Classification Algorithms",
  "number": 71,
  "duration": "10 min read",
  "content": "## Introduction to Logistic Regression\n\nLogistic Regression is a fundamental classification algorithm that predicts the probability of a binary outcome. Despite its name, it's used for classification, not regression.\n\n## The Logistic Function (Sigmoid)\n\nThe sigmoid function maps any real number to a value between 0 and 1:\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef sigmoid(z):\n    return 1 / (1 + np.exp(-z))\n\n# Visualize sigmoid function\nz = np.linspace(-10, 10, 100)\nplt.plot(z, sigmoid(z))\nplt.xlabel('z')\nplt.ylabel('sigmoid(z)')\nplt.title('Sigmoid Function')\nplt.axhline(y=0.5, color='r', linestyle='--')\nplt.grid(True)\nplt.show()\n```\n\n## How Logistic Regression Works\n\nLogistic regression combines linear regression with the sigmoid function:\n\n1. **Linear combination**: z = w₀ + w₁x₁ + w₂x₂ + ... + wₙxₙ\n2. **Apply sigmoid**: P(y=1|x) = σ(z) = 1/(1 + e^(-z))\n3. **Decision boundary**: Classify as 1 if P ≥ 0.5, else 0\n\n## Implementation with Scikit-learn\n\n```python\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Load dataset\ndata = load_breast_cancer()\nX, y = data.data, data.target\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Create and train model\nmodel = LogisticRegression(max_iter=10000)\nmodel.fit(X_train, y_train)\n\n# Make predictions\ny_pred = model.predict(X_test)\ny_prob = model.predict_proba(X_test)[:, 1]\n\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred))\n```\n\n## Understanding the Coefficients\n\n```python\nimport pandas as pd\n\n# Get feature importance\ncoef_df = pd.DataFrame({\n    'Feature': data.feature_names,\n    'Coefficient': model.coef_[0]\n})\ncoef_df['Abs_Coefficient'] = abs(coef_df['Coefficient'])\ncoef_df = coef_df.sort_values('Abs_Coefficient', ascending=False)\n\nprint(\"Top 10 Most Important Features:\")\nprint(coef_df.head(10))\n```\n\n## Regularization in Logistic Regression\n\nRegularization prevents overfitting by adding a penalty term:\n\n```python\n# L1 Regularization (Lasso) - creates sparse models\nmodel_l1 = LogisticRegression(penalty='l1', solver='saga', max_iter=10000)\nmodel_l1.fit(X_train, y_train)\n\n# L2 Regularization (Ridge) - default\nmodel_l2 = LogisticRegression(penalty='l2', max_iter=10000)\nmodel_l2.fit(X_train, y_train)\n\n# Elastic Net - combination of L1 and L2\nmodel_elastic = LogisticRegression(\n    penalty='elasticnet', \n    solver='saga', \n    l1_ratio=0.5,\n    max_iter=10000\n)\nmodel_elastic.fit(X_train, y_train)\n\nprint(f\"L1 Accuracy: {model_l1.score(X_test, y_test):.4f}\")\nprint(f\"L2 Accuracy: {model_l2.score(X_test, y_test):.4f}\")\nprint(f\"Elastic Net Accuracy: {model_elastic.score(X_test, y_test):.4f}\")\n```\n\n## Tuning the Regularization Strength\n\n```python\nfrom sklearn.model_selection import GridSearchCV\n\n# Define parameter grid\nparam_grid = {\n    'C': [0.001, 0.01, 0.1, 1, 10, 100],  # Inverse of regularization strength\n    'penalty': ['l1', 'l2'],\n    'solver': ['saga']\n}\n\n# Grid search\ngrid_search = GridSearchCV(\n    LogisticRegression(max_iter=10000),\n    param_grid,\n    cv=5,\n    scoring='accuracy'\n)\ngrid_search.fit(X_train, y_train)\n\nprint(f\"Best parameters: {grid_search.best_params_}\")\nprint(f\"Best CV score: {grid_search.best_score_:.4f}\")\n```\n\n## Decision Boundary Visualization\n\n```python\nfrom sklearn.decomposition import PCA\n\n# Reduce to 2D for visualization\npca = PCA(n_components=2)\nX_2d = pca.fit_transform(X)\n\n# Train on 2D data\nmodel_2d = LogisticRegression(max_iter=10000)\nmodel_2d.fit(X_2d, y)\n\n# Create mesh grid\nx_min, x_max = X_2d[:, 0].min() - 1, X_2d[:, 0].max() + 1\ny_min, y_max = X_2d[:, 1].min() - 1, X_2d[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n                     np.arange(y_min, y_max, 0.1))\n\n# Predict on mesh\nZ = model_2d.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\n# Plot\nplt.contourf(xx, yy, Z, alpha=0.4)\nplt.scatter(X_2d[:, 0], X_2d[:, 1], c=y, alpha=0.8)\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.title('Logistic Regression Decision Boundary')\nplt.show()\n```\n\n## Probability Calibration\n\n```python\nfrom sklearn.calibration import calibration_curve\n\n# Get probabilities\ny_prob = model.predict_proba(X_test)[:, 1]\n\n# Calculate calibration curve\nfraction_of_positives, mean_predicted_value = calibration_curve(\n    y_test, y_prob, n_bins=10\n)\n\n# Plot\nplt.figure(figsize=(8, 6))\nplt.plot(mean_predicted_value, fraction_of_positives, 's-', label='Logistic')\nplt.plot([0, 1], [0, 1], '--', label='Perfectly calibrated')\nplt.xlabel('Mean Predicted Probability')\nplt.ylabel('Fraction of Positives')\nplt.title('Calibration Curve')\nplt.legend()\nplt.show()\n```\n\n## Best Practices\n\n1. **Scale your features**: Logistic regression is sensitive to feature scales\n2. **Handle multicollinearity**: Correlated features can cause unstable coefficients\n3. **Check for class imbalance**: Use `class_weight='balanced'` if needed\n4. **Start simple**: Use L2 regularization before trying L1\n5. **Interpret coefficients carefully**: They represent log-odds, not probabilities",
  "code": "# Logistic Regression - Complete Example\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.metrics import accuracy_score, classification_report\nimport numpy as np\n\n# Load the breast cancer dataset\ndata = load_breast_cancer()\nX, y = data.data, data.target\nprint(f\"Dataset shape: {X.shape}\")\nprint(f\"Classes: {data.target_names}\")\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Scale features (important for logistic regression)\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Create and train the model\nmodel = LogisticRegression(max_iter=1000, random_state=42)\nmodel.fit(X_train_scaled, y_train)\n\n# Make predictions\ny_pred = model.predict(X_test_scaled)\ny_prob = model.predict_proba(X_test_scaled)\n\n# Evaluate\nprint(f\"\\nAccuracy: {accuracy_score(y_test, y_pred):.4f}\")\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred, target_names=data.target_names))\n\n# Show prediction probabilities for first 5 samples\nprint(\"\\nPrediction probabilities (first 5 samples):\")\nfor i in range(5):\n    print(f\"Sample {i}: {data.target_names[0]}={y_prob[i][0]:.3f}, {data.target_names[1]}={y_prob[i][1]:.3f} -> Predicted: {data.target_names[y_pred[i]]}\")"
}
