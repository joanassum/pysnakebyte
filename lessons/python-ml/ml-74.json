{
  "id": "ml-74",
  "title": "Decision Tree Classification",
  "category": "Classification Algorithms",
  "number": 74,
  "duration": "12 min read",
  "content": "## Decision Tree Classification\n\nDecision Trees are versatile algorithms that can perform both classification and regression. They learn simple decision rules inferred from data features to predict the target.\n\n### Structure\n\n*   **Root Node**: Represents the entire population or sample.\n*   **Decision Node**: A sub-node that splits into further sub-nodes.\n*   **Leaf/Terminal Node**: Nodes that do not split (final outcome).\n\n### Splitting Criteria\n\nHow does the tree decide where to split?\n1.  **Gini Impurity**: Measures the likelihood of an incorrect classification of a new instance of a random variable, if that new instance were randomly classified according to the distribution of class labels from the data set. (Default in sklearn).\n2.  **Entropy (Information Gain)**: Measures the amount of information or disorder/uncertainty in a random variable.\n\n### Overfitting\n\nDecision trees are prone to overfitting (creating overly complex trees). We use **pruning** or set hyperparameters like `max_depth`, `min_samples_split`, and `min_samples_leaf` to control complexity.\n\n```python\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\n\n# Restricting depth to prevent overfitting\ntree_clf = DecisionTreeClassifier(max_depth=3, random_state=42)\ntree_clf.fit(X_train, y_train)\n```\n",
  "code": "from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.datasets import load_wine\nfrom sklearn.model_selection import train_test_split\n\ndata = load_wine()\nX, y = data.data, data.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# TODO: Initialize DecisionTreeClassifier with max_depth=3 to prevent overfitting\ndt = \n\n# TODO: Fit the model\n\n# TODO: Predict on test data\n\n# Check the feature importances\n# print(\"Feature importances:\", dt.feature_importances_)"
}