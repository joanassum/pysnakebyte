{
  "id": "ml-45",
  "title": "Feature Engineering",
  "category": "ML Fundamentals",
  "number": 45,
  "duration": "11 min read",
  "content": "## What is Feature Engineering?\n\nFeature engineering is the process of creating new features or transforming existing ones to improve model performance. It's often the most impactful part of ML.\n\n## Why Feature Engineering Matters\n\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\n# Example: Predicting with raw features vs engineered features\nnp.random.seed(42)\n\n# Raw data: store sales\nn = 1000\ndf = pd.DataFrame({\n    'date': pd.date_range('2022-01-01', periods=n),\n    'store_id': np.random.choice([1, 2, 3, 4, 5], n),\n    'temperature': np.random.normal(70, 15, n),\n    'is_weekend': np.random.choice([0, 1], n),\n    'competitor_promo': np.random.choice([0, 1], n)\n})\n\n# True relationship includes engineered features\ndf['sales'] = (\n    1000 + \n    50 * df['is_weekend'] + \n    200 * df['date'].dt.month.isin([11, 12]).astype(int) +  # Holiday season\n    -5 * (df['temperature'] - 70) ** 2 / 100 +  # Non-linear temp effect\n    -100 * df['competitor_promo'] +\n    np.random.normal(0, 50, n)\n)\n\nprint(\"Raw Features Only:\")\nX_raw = df[['temperature', 'is_weekend', 'competitor_promo']].values\ny = df['sales']\n\nX_train, X_test, y_train, y_test = train_test_split(X_raw, y, test_size=0.2, random_state=42)\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nprint(f\"R² Score: {r2_score(y_test, model.predict(X_test)):.4f}\")\n```\n\n## Types of Feature Engineering\n\n### 1. Numerical Transformations\n\n```python\n# Numerical transformations\ndf_features = pd.DataFrame()\n\n# Original feature\ndf_features['temperature'] = df['temperature']\n\n# Polynomial features\ndf_features['temp_squared'] = df['temperature'] ** 2\ndf_features['temp_cubed'] = df['temperature'] ** 3\n\n# Log transformation (for skewed data)\ndf_features['log_sales'] = np.log1p(df['sales'])  # log1p handles zeros\n```",
  "code": "import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import r2_score, mean_absolute_error\n\n# Feature Engineering Practice\nnp.random.seed(42)\n\n# E-commerce transaction data\nn = 1000\ndf = pd.DataFrame({\n    'timestamp': pd.date_range('2023-01-01', periods=n, freq='h'),\n    'price': np.random.uniform(10, 200, n),\n    'quantity': np.random.poisson(2, n) + 1,\n    'category': np.random.choice(['electronics', 'clothing', 'food'], n),\n    'customer_age': np.random.randint(18, 70, n)\n})\n\n# Target: total revenue (to predict)\ndf['revenue'] = df['price'] * df['quantity']\n\nprint(\"Original Features:\")\nprint(df.head())\n\n# YOUR TASK: Create new features\n# 1. Extract datetime features\ndf['hour'] = df['timestamp'].dt.hour\ndf['day_of_week'] = df['timestamp'].dt.dayofweek\ndf['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n\n# 2. Create customer segments\ndf['age_group'] = pd.cut(df['customer_age'], \n                         bins=[0, 25, 40, 55, 100],\n                         labels=['young', 'adult', 'middle', 'senior'])\n\n# 3. One-hot encode categories\ndf = pd.get_dummies(df, columns=['category', 'age_group'])\n\n# 4. Interaction feature\ndf['price_qty_interaction'] = df['price'] * np.log1p(df['quantity'])\n\nprint(f\"\\nFeatures after engineering: {df.shape[1]} columns\")\n\n# Evaluate improvement\nfeature_cols = [c for c in df.columns if c not in ['timestamp', 'revenue']]\nX = df[feature_cols]\ny = df['revenue']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = RandomForestRegressor(n_estimators=100, random_state=42)\nmodel.fit(X_train, y_train)\n\nprint(f\"\\nModel R²: {r2_score(y_test, model.predict(X_test)):.4f}\")\n\n# Try: Add more features like hour bins or price percentiles"
}
