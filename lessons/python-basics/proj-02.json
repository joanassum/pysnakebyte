{
  "id": "proj-02",
  "title": "Web Scraping Basics",
  "category": "Bonus: Real-World Projects",
  "number": 102,
  "duration": "15 min read",
  "content": "## Project Overview\n\nLearn how to extract data from websites using Python. You'll build a web scraper that can fetch webpage content, parse HTML, and extract useful information.\n\n## Learning Objectives\n\n- Understand HTTP requests and responses\n- Parse HTML with BeautifulSoup\n- Extract and structure data from web pages\n- Handle errors and edge cases in web scraping\n\n## Prerequisites\n\nInstall the required libraries:\n\n```bash\npip install requests beautifulsoup4\n```\n\n## Step 1: Making HTTP Requests\n\nStart by learning how to fetch web pages:\n\n```python\nimport requests\n\ndef fetch_page(url):\n    \"\"\"Fetch a webpage and return its content.\"\"\"\n    try:\n        response = requests.get(url, timeout=10)\n        response.raise_for_status()  # Raise exception for bad status codes\n        return response.text\n    except requests.RequestException as e:\n        print(f\"Error fetching {url}: {e}\")\n        return None\n\n# Example usage\nhtml = fetch_page(\"https://example.com\")\nif html:\n    print(f\"Fetched {len(html)} characters\")\n```\n\n## Step 2: Parsing HTML with BeautifulSoup\n\nBeautifulSoup makes it easy to navigate and search HTML:\n\n```python\nfrom bs4 import BeautifulSoup\n\ndef parse_html(html_content):\n    \"\"\"Parse HTML content and return a BeautifulSoup object.\"\"\"\n    return BeautifulSoup(html_content, 'html.parser')\n\n# Finding elements\nsoup = parse_html(html)\n\n# Find by tag\nall_links = soup.find_all('a')\nfirst_paragraph = soup.find('p')\n\n# Find by class\nitems = soup.find_all('div', class_='item')\n\n# Find by ID\nheader = soup.find(id='header')\n\n# CSS selectors\narticles = soup.select('article.post')\n```\n\n## Step 3: Extracting Data\n\nLearn to extract text and attributes:\n\n```python\ndef extract_links(soup):\n    \"\"\"Extract all links from a page.\"\"\"\n    links = []\n    for a_tag in soup.find_all('a', href=True):\n        links.append({\n            'text': a_tag.get_text(strip=True),\n            'url': a_tag['href']\n        })\n    return links\n\ndef extract_headings(soup):\n    \"\"\"Extract all headings from a page.\"\"\"\n    headings = []\n    for level in range(1, 7):\n        for heading in soup.find_all(f'h{level}'):\n            headings.append({\n                'level': level,\n                'text': heading.get_text(strip=True)\n            })\n    return headings\n```\n\n## Step 4: Building a News Scraper\n\nLet's build a practical example - a news headline scraper:\n\n```python\nimport requests\nfrom bs4 import BeautifulSoup\nfrom datetime import datetime\n\nclass NewsScraper:\n    \"\"\"A simple news headline scraper.\"\"\"\n    \n    def __init__(self):\n        self.headers = {\n            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n        }\n    \n    def fetch(self, url):\n        \"\"\"Fetch a webpage with proper headers.\"\"\"\n        try:\n            response = requests.get(url, headers=self.headers, timeout=10)\n            response.raise_for_status()\n            return BeautifulSoup(response.text, 'html.parser')\n        except requests.RequestException as e:\n            print(f\"Error: {e}\")\n            return None\n    \n    def scrape_headlines(self, url, headline_selector):\n        \"\"\"Scrape headlines using a CSS selector.\"\"\"\n        soup = self.fetch(url)\n        if not soup:\n            return []\n        \n        headlines = []\n        for element in soup.select(headline_selector):\n            text = element.get_text(strip=True)\n            if text:\n                headlines.append(text)\n        \n        return headlines\n```\n\n## Step 5: Saving Scraped Data\n\nSave your scraped data to files:\n\n```python\nimport json\nimport csv\n\ndef save_to_json(data, filename):\n    \"\"\"Save data to a JSON file.\"\"\"\n    with open(filename, 'w', encoding='utf-8') as f:\n        json.dump(data, f, indent=2, ensure_ascii=False)\n    print(f\"Saved {len(data)} items to {filename}\")\n\ndef save_to_csv(data, filename, fieldnames):\n    \"\"\"Save data to a CSV file.\"\"\"\n    with open(filename, 'w', newline='', encoding='utf-8') as f:\n        writer = csv.DictWriter(f, fieldnames=fieldnames)\n        writer.writeheader()\n        writer.writerows(data)\n    print(f\"Saved {len(data)} items to {filename}\")\n```\n\n## Step 6: Handling Pagination\n\nMany websites have multiple pages of content:\n\n```python\ndef scrape_paginated(base_url, page_param, max_pages=5):\n    \"\"\"Scrape multiple pages of content.\"\"\"\n    all_data = []\n    \n    for page in range(1, max_pages + 1):\n        url = f\"{base_url}?{page_param}={page}\"\n        print(f\"Scraping page {page}...\")\n        \n        soup = fetch_and_parse(url)\n        if not soup:\n            break\n        \n        page_data = extract_items(soup)\n        if not page_data:\n            break  # No more data\n        \n        all_data.extend(page_data)\n        time.sleep(1)  # Be polite - wait between requests\n    \n    return all_data\n```\n\n## Best Practices\n\n1. **Respect robots.txt**: Check if scraping is allowed\n2. **Add delays**: Don't overwhelm servers with requests\n3. **Use headers**: Include a User-Agent to identify your scraper\n4. **Handle errors**: Websites can be unreliable\n5. **Cache results**: Save data to avoid re-scraping\n\n```python\nimport time\nimport random\n\ndef polite_scrape(urls, delay_range=(1, 3)):\n    \"\"\"Scrape URLs with random delays.\"\"\"\n    results = []\n    for url in urls:\n        result = fetch_page(url)\n        results.append(result)\n        \n        # Random delay between requests\n        delay = random.uniform(*delay_range)\n        time.sleep(delay)\n    \n    return results\n```\n\n## Challenge Extensions\n\n1. **Image Scraper**: Download all images from a webpage\n2. **Price Tracker**: Monitor product prices and alert on changes\n3. **Job Board Scraper**: Collect job listings from multiple sites\n4. **Data Pipeline**: Scrape, clean, and store data in a database",
  "code": "# Web Scraper - Complete Implementation\nimport requests\nfrom bs4 import BeautifulSoup\nimport json\nimport csv\nimport time\nfrom datetime import datetime\n\nclass WebScraper:\n    \"\"\"A versatile web scraping toolkit.\"\"\"\n    \n    def __init__(self, delay=1.0):\n        \"\"\"Initialize scraper with configurable delay.\"\"\"\n        self.delay = delay\n        self.headers = {\n            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n        }\n        self.session = requests.Session()\n        self.session.headers.update(self.headers)\n    \n    def fetch(self, url):\n        \"\"\"Fetch a webpage and return BeautifulSoup object.\"\"\"\n        try:\n            response = self.session.get(url, timeout=10)\n            response.raise_for_status()\n            time.sleep(self.delay)  # Be polite\n            return BeautifulSoup(response.text, 'html.parser')\n        except requests.RequestException as e:\n            print(f\"Error fetching {url}: {e}\")\n            return None\n    \n    def extract_links(self, soup, base_url=\"\"):\n        \"\"\"Extract all links from a page.\"\"\"\n        links = []\n        for a_tag in soup.find_all('a', href=True):\n            href = a_tag['href']\n            # Convert relative URLs to absolute\n            if href.startswith('/'):\n                href = base_url + href\n            links.append({\n                'text': a_tag.get_text(strip=True),\n                'url': href\n            })\n        return links\n    \n    def extract_text(self, soup, selector):\n        \"\"\"Extract text from elements matching a CSS selector.\"\"\"\n        elements = soup.select(selector)\n        return [el.get_text(strip=True) for el in elements]\n    \n    def extract_table(self, soup, table_selector='table'):\n        \"\"\"Extract data from an HTML table.\"\"\"\n        table = soup.select_one(table_selector)\n        if not table:\n            return []\n        \n        rows = []\n        headers = [th.get_text(strip=True) for th in table.select('th')]\n        \n        for tr in table.select('tr'):\n            cells = [td.get_text(strip=True) for td in tr.select('td')]\n            if cells:\n                if headers:\n                    rows.append(dict(zip(headers, cells)))\n                else:\n                    rows.append(cells)\n        \n        return rows\n    \n    def scrape_multiple(self, urls):\n        \"\"\"Scrape multiple URLs.\"\"\"\n        results = []\n        for i, url in enumerate(urls, 1):\n            print(f\"Scraping {i}/{len(urls)}: {url}\")\n            soup = self.fetch(url)\n            if soup:\n                results.append({\n                    'url': url,\n                    'title': soup.title.string if soup.title else '',\n                    'soup': soup\n                })\n        return results\n\n\ndef save_to_json(data, filename):\n    \"\"\"Save data to JSON file.\"\"\"\n    with open(filename, 'w', encoding='utf-8') as f:\n        json.dump(data, f, indent=2, ensure_ascii=False)\n    print(f\"Saved to {filename}\")\n\n\ndef save_to_csv(data, filename):\n    \"\"\"Save list of dicts to CSV file.\"\"\"\n    if not data:\n        return\n    \n    fieldnames = data[0].keys()\n    with open(filename, 'w', newline='', encoding='utf-8') as f:\n        writer = csv.DictWriter(f, fieldnames=fieldnames)\n        writer.writeheader()\n        writer.writerows(data)\n    print(f\"Saved to {filename}\")\n\n\n# Example Usage\nif __name__ == \"__main__\":\n    scraper = WebScraper(delay=1.0)\n    \n    # Fetch a page\n    soup = scraper.fetch(\"https://example.com\")\n    \n    if soup:\n        # Extract title\n        print(f\"Page Title: {soup.title.string}\")\n        \n        # Extract all links\n        links = scraper.extract_links(soup, \"https://example.com\")\n        print(f\"Found {len(links)} links\")\n        \n        # Save results\n        save_to_json(links, \"scraped_links.json\")\n        \n        print(\"Scraping complete!\")"
}
